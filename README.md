# gptsec
Make ML more secure.

| Category | Subcategory | Description |
| --- | --- | --- |
| Data security | |
| |	Data leakage | Unauthorized access or exposure of sensitive training and testing data. |
| | Data tampering |	Unauthorized modification of data, leading to biased or incorrect model predictions. |
| | Privacy breaches | Failure to protect personally identifiable information (PII) or adhere to data protection regulations like GDPR or CCPA. |
| Model Security	| |
| | Model stealing | Unauthorized access to and theft of trained machine learning models. |
| | Model inversion | Inferring sensitive information about the training data from the model's output. |
| | Model poisoning	| Manipulating the training data or model to cause incorrect or malicious predictions. |
| Infrastructure	| |
| | Insecure APIs / endpoints | APIs and endpoints can allow attackers to compromise systems and gain unauthorized access. |
| General	| |
| | Insecure containerization	| Weak containerization practices or misconfigurations can lead to security vulnerabilities. |
| | Weak authentication and authorization	| Inadequate access controls can result in unauthorized access to MLOps systems and resources. |
| Supply Chain Security | |
| | Dependency vulnerabilities |	Using third-party libraries or frameworks with known security vulnerabilities. |
| | Code tampering |	Unauthorized modification of code, which can result in malicious behavior or compromise of the system. |
| | Insecure model sharing | Exchanging models or data without proper security measures, which can lead to vulnerabilities and exposure. |
| Adversarial Attacks	| |
| | Evasion attacks |	Manipulating input data to mislead a machine learning model into producing incorrect predictions. |
| | Poisoning attacks |	Injecting malicious data into the training set to compromise the model's performance or induce specific malicious behaviors. |
| | Model extraction attacks | Exploiting the model to extract intellectual property or sensitive information. |
